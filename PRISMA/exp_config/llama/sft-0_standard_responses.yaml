exp_name: llama-2
trainer:
  run_name: sft-0
  output_dir: ft_models_standard/llama-2/sft-0
  overwrite_output_dir: true
  bf16: true
  tf32: true
  report_to: 'wandb'
  seed: 42
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 2
  num_train_epochs: 2
  optim: 'adamw_torch'
  weight_decay: 0.01
  lr_scheduler_type: 'cosine_with_min_lr'
  learning_rate: 2e-5
  lr_scheduler_kwargs:
    min_lr_rate: 0.1
  warmup_ratio: 0.1
  save_strategy: 'no'
  ddp_find_unused_parameters: false
  logging_steps: 10
  dataloader_num_workers: 12

model:
  model_path: 'hf_models/llama-2'

data:
  data_path: 'gsm8k/train.jsonl'
  train_data_path: 'gsm8k/train.jsonl'
  dev_data_path: 'gsm8k/dev.jsonl'
  test_data_path: 'gsm8k/test.jsonl'
  tokenizer_path: ${model.model_path}
  model_max_length: 640
  max_src_len: 256
  max_tgt_len: 384

eval:
  per_device_eval_batch_size: 2
  max_new_tokens: 300
  use_calc: true
  mode: 'greedy'
  sampling:
    temperature: 0.7
    min_seed: 0
    max_seed: 10



